# -*- coding: utf-8 -*-
"""97.44%

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wGCvTSBnSuNKxrDu9clrXF0WuUWlUto5
"""

!pip install imblearn
import os
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
from distutils.dir_util import copy_tree, remove_tree
from PIL import Image
from imblearn.over_sampling import ADASYN

from sklearn.model_selection import train_test_split
from sklearn.metrics import matthews_corrcoef as MCC
from sklearn.metrics import balanced_accuracy_score as BAS
from sklearn.metrics import classification_report, confusion_matrix

from keras.utils.vis_utils import plot_model
from keras.regularizers import l2

!pip install --upgrade tensorflow
from tensorflow.keras import Sequential, Input
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Dense, Dropout,Conv2D, Flatten
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.preprocessing.image import ImageDataGenerator as IDG
from tensorflow.keras.layers import SeparableConv2D, BatchNormalization, MaxPool2D,GlobalAveragePooling2D, Flatten
from tensorflow.keras.models import Model

from google.colab import drive
drive.mount('/content/drive')

#Define the directories containing the images
test_folder = "/content/drive/MyDrive/archive/Alzheimer_s Dataset/test/"
train_folder= "/content/drive/MyDrive/archive/Alzheimer_s Dataset/train/"

#Define the list of classes
My_classes = ['NonDemented', 'VeryMildDemented', 'MildDemented', 'ModerateDemented']

#Initialize an empty dictionary to hold the results
obtained_results = {'Path': [], 'Class_name': [], ' Images Numbers ': []}

# Loop through the directories and count the number of images in each class
for directory in [test_folder, train_folder]:
    for class_name in obtained_results:
        class_dir = os.path.join(directory, class_name)
        num_images = len(os.listdir(class_dir))
        obtained_results['Directory'].append(directory)
        obtained_results['Class'].append(class_name)
        obtained_results['Number of images'].append(num_images)

# Convert the results to a Pandas dataframe and display as a table
resultsDataFrame = pd.DataFrame.from_dict(obtained_results)
print(resultsDataFrame)

New_dataset = "/content/drive/MyDrive/archive/pp/"

if os.path.exists(New_dataset):
    remove_tree(New_dataset)
    

os.mkdir(New_dataset)
copy_tree(train_folder, New_dataset)
copy_tree(train_folder, New_dataset)
print("Working Directory Contents:", os.listdir(New_dataset))

# Define the path to the subfolders containing the class subfolders
class_directory = New_dataset

# Loop over the class subfolders
for name_of_class in os.listdir(class_directory):
    class_folder = os.path.join(class_directory, name_of_class)
    if os.path.isdir(class_folder):
        # Print the number of images in the class subfolder
        images_number = len(os.listdir(class_folder))
        print(f"{name_of_class}: {images_number} images")
        # Loop over each file in the class subfolder
        for image in os.listdir(class_folder):
            filepath = os.path.join(class_folder, image)
            if image.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
                # Open the image and get its size, number of channels, and coding bites
                with Image.open(filepath) as img:
                    size = img.size
                    channels = img.getbands()
                    pixel_bits = img.bits
                    print(f"{class_name} {image}: {size} pixels, {channels}, {pixel_bits} bits")

# Get a list of all subfolders in the directory
subfolders_names = [f.path for f in os.scandir(New_dataset) if f.is_dir()]

# Loop over the subfolders and count the number of files in each one
Totals = []
for subfolder in subfolders_names:
    images_numberr = len(os.listdir(subfolder))
    print("{}: {}".format(os.path.basename(subfolder), images_numberr))
    Totals.append(images_numberr)

# Plot the results as a bar chart
plt.bar(range(len(subfolders_names)), Totals, width=0.3,color="pink")
plt.xticks(range(len(subfolders_names)), [os.path.basename(s) for s in subfolders_names], fontsize=7)
plt.xlabel("Classes Names  ")
plt.ylabel("Images Number")
plt.title("Images Distribution".format(New_dataset))
plt.show()

adasyn = ADASYN()

our_classes = ['NonDemented','VeryMildDemented','MildDemented','ModerateDemented']
size = 160
size_dimensions= (size, size)

#Performing Image Augmentation to have more data samples
ZOOMING = [.99, 1.01]
HORZ_FLIPPING = True
BRIGHTNESS = [0.8, 1.2]
FILL_MODE = "constant"
DATA_FORMAT = "channels_last"

My_IDG_PARAMETERS = IDG(rescale = 1./255,zoom_range=ZOOMING, brightness_range=BRIGHTNESS, data_format=DATA_FORMAT, fill_mode=FILL_MODE, horizontal_flip=HORZ_FLIPPING)
MY_train_IDG = My_IDG_PARAMETERS.flow_from_directory(directory= New_dataset, target_size= size_dimensions, batch_size=6420, shuffle=False)

#Retrieving the data from the ImageDataGenerator iterator

train_data, train_labels = MY_train_IDG.next()

Oversampled_train_X, Oversampled_train_Y = adasyn.fit_resample(train_data.reshape(-1,160 * 160 * 3), train_labels)
Oversampled_train_X= Oversampled_train_X.reshape(-1,size, size, 3)

print(Oversampled_train_X.shape, Oversampled_train_Y.shape)

#Splitting the data into train, test, and validation sets

train_imgs, test_imgs, train_labels , test_labels = train_test_split(Oversampled_train_X, Oversampled_train_Y, test_size = 0.1, random_state=42)
train_imgs, val_imgs, train_labels, val_labels = train_test_split(Oversampled_train_X, Oversampled_train_Y, test_size = 0.2, random_state=42)

print("Number of samples in training set:", len(train_imgs))
print("Number of samples in validation set:", len(val_imgs))
print("Number of samples in testing set:", len( test_imgs))
# Define data
labels = ['Training Set', 'Validation Set', 'Testing Set']
sizes = [len(train_imgs), len(val_imgs), len( test_imgs)]
colors = ['#FFC0CB', '#808000', '#ADD8E6']  # Pink, Olive Green, Light Blue

# Create pie chart
fig, ax = plt.subplots()
ax.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle

# Add title
plt.title('Dataset Distribution')

# Show plot
plt.show()

#Get the number of samples in each class for the training set
train_counts = [np.sum(np.argmax(train_labels, axis=1) == i) for i in range(len(our_classes))]
print("Training set class counts:", train_counts)

#Get the number of samples in each class for the validation set
val_counts = [np.sum(np.argmax(val_labels, axis=1) == i) for i in range(len(our_classes))]
print("Validation set class counts:", val_counts)

#Get the number of samples in each class for the testing set
test_counts = [np.sum(np.argmax(test_labels, axis=1) == i) for i in range(len(our_classes))]
print("Testing set class counts:", test_counts)

#Set the x-axis positions for the bars
x_positions = np.arange(len(our_classes))

#Plot the class counts for the training set
plt.bar(x_positions - 0.1, train_counts, width=0.1, color='pink', label='Training Set')

#Plot the class counts for the validation set
plt.bar(x_positions, val_counts, width=0.1, color='green', label='Validation Set')

#Plot the class counts for the testing set
plt.bar(x_positions + 0.1, test_counts, width=0.1, color='purple', label='Testing Set')

# Add a title and legend to the plot
plt.title('Class distribution')
plt.xticks(x_positions, our_classes)
plt.legend()

#Show the plot
plt.show()

from tensorflow.keras.applications import *
INCEP= InceptionV3(input_shape=(size, size, 3), include_top=False, weights="imagenet")

for layer in INCEP.layers [:40]:
    layer.trainable = False

INCEP.summary()

Drop = 0.5
My_model = Sequential([
        INCEP,
        Dropout(Drop),
        GlobalAveragePooling2D(),
        BatchNormalization(),
        Flatten(),
        Dense(18432, activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        Dropout(Drop),
        Dense(500, activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(),
        Dropout(Drop),
        Dense(4, activation='softmax')        
    ], name = "cnn_model")
My_model.summary()

# Define early stopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3)

# Define learning rate scheduler
Learning_Rate_Reduction_Callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.1)

Overfitting_prevent= [early_stopping, Learning_Rate_Reduction_Callback]

METRICS = [tf.keras.metrics.CategoricalAccuracy(name='acc')]

    
My_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                              loss=tf.losses.CategoricalCrossentropy(),
                              metrics=METRICS)


# Fit the training data to the model and validate it using the validation data
history = My_model.fit(train_imgs, train_labels, validation_data=(val_imgs, val_labels), epochs=30,callbacks=Overfitting_prevent)
My_model.save('/content/drive/MyDrive/archive/Alzheimer_s Dataset/model.h5')

# Plotting the trend of the metrics during training
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Plotting Accuracy
ax[0].plot(history.history['acc'], label='train')
ax[0].plot(history.history['val_acc'], label='val')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('Accuracy')
ax[0].set_title('Model Accuracy')
ax[0].legend()

# Plotting Loss
ax[1].plot(history.history['loss'], label='train')
ax[1].plot(history.history['val_loss'], label='val')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Loss')
ax[1].set_title('Model Loss')
ax[1].legend()

plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report

train_scores = My_model.evaluate(train_imgs, train_labels)
validation_scores = My_model.evaluate(val_imgs, val_labels)
test_scores = My_model.evaluate(test_imgs, test_labels)

print("Training Accuracy: %.2f%%"%(train_scores[1] * 100))
print("Validation Accuracy: %.2f%%"%(validation_scores[1] * 100))
print("Testing Accuracy: %.2f%%"%(test_scores[1] * 100))

# Predicting the test data
x_prediction = My_model.predict(test_imgs)

test_ls = np.argmax(test_labels, axis=1)
pred_ls = np.argmax(x_prediction, axis=1)

# Classification report to get precision, recall, f1-score and support
print('Classification Report')
print(classification_report(test_ls, pred_ls, target_names= our_classes))

# Plot the confusion matrix to understand the classification in detail
conf_arr = confusion_matrix(test_ls, pred_ls)

plt.figure(figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')
ax = sns.heatmap(conf_arr, cmap='Greens', annot=True, fmt='d', xticklabels= our_classes, yticklabels= our_classes)
plt.title('Alzheimer\'s Disease Diagnosis')
plt.xlabel('Prediction')
plt.ylabel('Truth')
plt.show()